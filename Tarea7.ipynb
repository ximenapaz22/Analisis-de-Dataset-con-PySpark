{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3eb01b3-c823-4990-b38c-7b0a0cdcb97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import tarfile\n",
    "import os\n",
    "import py4j.protocol  \n",
    "from py4j.protocol import Py4JJavaError  \n",
    "from py4j.java_gateway import JavaObject  \n",
    "from py4j.java_collections import JavaArray, JavaList\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark import RDD, SparkContext  \n",
    "from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n",
    "from pyspark.sql.types import StringType,ByteType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a4536ff-abcc-4abc-8d25-e96ecbb3f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "897bacad-a5c3-4619-abc4-e41ec7d82280",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_archivo_tar = \"/home/jovyan/work/yelp_dataset.tar\"\n",
    "\n",
    "directorio_extraccion = \"/home/jovyan/work/yelp_dataset/\" \n",
    "\n",
    "with tarfile.open(ruta_archivo_tar, \"r\") as tar:\n",
    "    tar.extractall(directorio_extraccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7be1685-1154-42d0-8ec5-fd811c2d3935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/work/yelp_dataset/yelp_academic_dataset_checkin.json',\n",
       " '/home/jovyan/work/yelp_dataset/Dataset_User_Agreement.pdf',\n",
       " '/home/jovyan/work/yelp_dataset/yelp_academic_dataset_tip.json',\n",
       " '/home/jovyan/work/yelp_dataset/review.parquet',\n",
       " '/home/jovyan/work/yelp_dataset/yelp_academic_dataset_review.json',\n",
       " '/home/jovyan/work/yelp_dataset/.ipynb_checkpoints',\n",
       " '/home/jovyan/work/yelp_dataset/yelp_academic_dataset_business.json',\n",
       " '/home/jovyan/work/yelp_dataset/yelp_academic_dataset_review.parquet',\n",
       " '/home/jovyan/work/yelp_dataset/yelp_academic_dataset_user.json']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archivos_extraidos = [os.path.join(directorio_extraccion, nombre) for nombre in os.listdir(directorio_extraccion)]\n",
    "archivos_extraidos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da61bfa1-8f34-4f1f-9c1c-649de3b6cbb5",
   "metadata": {},
   "source": [
    "1. Toma el archivo `review.json` JSON y cuantífica cuánto pesa el archivo en   disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8434f237-8fde-4ea0-bada-8328e5a5a2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del archivo review.json es aproximadamente 5.34 GB\n"
     ]
    }
   ],
   "source": [
    "ruta_archivo_json = \"/home/jovyan/work/yelp_dataset/yelp_academic_dataset_review.json\"\n",
    "tamaño_bytes = os.path.getsize(ruta_archivo_json)\n",
    "tamaño_gb = tamaño_bytes / (1000 * 1000 * 1000)\n",
    "print(f\"El tamaño del archivo review.json es aproximadamente {tamaño_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6e1e5-4be0-4681-8cc5-b93be3462c30",
   "metadata": {},
   "source": [
    "2. Carga el JSON en Spark y cuantífica cuánto pesa el DataFramen en memoria RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97f053cd-ebef-40a8-bba2-d41cc8202ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|         business_id|cool|               date|funny|           review_id|stars|                text|useful|             user_id|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|XQfwVwDr-v0ZS3_Cb...|   0|2018-07-07 22:09:11|    0|KU_O5udG6zpxOg-Vc...|  3.0|If you decide to ...|     0|mh_-eMZ6K5RLWhZyI...|\n",
      "|7ATYjTIgM3jUlt4UM...|   1|2012-01-03 15:28:18|    0|BiTunyQ73aT9WBnpR...|  5.0|I've taken a lot ...|     1|OyoGAe7OKpv6SyGZT...|\n",
      "|YjUWPpI6HXG530lwP...|   0|2014-02-05 20:30:30|    0|saUsX_uimxRlCVr67...|  3.0|Family diner. Had...|     0|8g_iMtfSiwikVnbP2...|\n",
      "|kxX2SOes4o-D3ZQBk...|   1|2015-01-04 00:01:03|    0|AqPFMleE6RsU23_au...|  5.0|Wow!  Yummy, diff...|     1|_7bHUi9Uuf5__HHc_...|\n",
      "|e4Vwtrqf-wpJfwesg...|   1|2017-01-14 20:54:15|    0|Sx8TMOWLNuJBWer-0...|  4.0|Cute interior and...|     1|bcjbaE6dDog4jkNY9...|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"/home/jovyan/work/yelp_dataset/yelp_academic_dataset_review.json\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23bd0685-97dc-4629-aa12-aa2221b1c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del DataFrame en memoria (RAM) es aproximadamente 5.34 GB\n"
     ]
    }
   ],
   "source": [
    "df_cached = df.cache()\n",
    "stats = df_cached._jdf.queryExecution().optimizedPlan().stats()\n",
    "df_ram_size_bytes = stats.sizeInBytes()\n",
    "df_ram_size_gb = df_ram_size_bytes / (1000 * 1000 * 1000)\n",
    "print(f\"El tamaño del DataFrame en memoria (RAM) es aproximadamente {df_ram_size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561d52bd-072d-4911-b4c6-93493b766a53",
   "metadata": {},
   "source": [
    "3. Guarda el DataFrame como parquet en disco y muestra cuánto pesa el archivo.\n",
    "   Cómo se compara con el JSON crudo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cf8a03-f585-4f65-9831-5eedf64c9760",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"/home/jovyan/work/yelp_dataset/yelp_academic_dataset_review.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d619b7a5-1ee6-4251-b127-4166095a4a87",
   "metadata": {},
   "source": [
    "El archivo pesa 5.34 en JSON y 3.12 en parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9432a7-e336-42c1-82f0-da4b8d249398",
   "metadata": {},
   "source": [
    "4. Utiliza el DataFrame, optimiza el tipo de dato que hay en cada columna (i.e. \n",
    "   `Int32`, `Int64`, `Float32`, `Float64`, `String`, `Categorical`) y guarda el \n",
    "   nuevo DataFrame como parquet. Cuántifica cuánto pesa el DataFrame en memoria \n",
    "   RAM y cuánto pesa en disco. Cómo se compara con el parquet crudo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7efa8eb-cd06-42c4-a342-96a05d563dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"stars\", func.col(\"stars\").cast(\"float\"))\n",
    "df = df.withColumn(\"useful\", func.col(\"useful\").cast(ByteType()))\n",
    "df = df.withColumn(\"funny\", func.col(\"funny\").cast(ByteType()))\n",
    "df = df.withColumn(\"cool\", func.col(\"cool\").cast(ByteType()))\n",
    "df = df.withColumn(\"date\", func.to_date(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "804c5a0f-17bc-4619-9b89-f6557df7001d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del DataFrame en memoria (RAM) es aproximadamente 3.78 GB\n"
     ]
    }
   ],
   "source": [
    "df_cached = df.cache()\n",
    "stats = df_cached._jdf.queryExecution().optimizedPlan().stats()\n",
    "df_ram_size_bytes = stats.sizeInBytes()\n",
    "df_ram_size_gb = df_ram_size_bytes / (1000 * 1000 * 1000)\n",
    "print(f\"El tamaño del DataFrame en memoria (RAM) es aproximadamente {df_ram_size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a058af88-9f8d-4ce2-92d1-89c3f1bcdda6",
   "metadata": {},
   "source": [
    "El tamaño en discto es 3GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17833eb-bf70-4668-a14b-b73956a7f0f6",
   "metadata": {},
   "source": [
    "5. Utiliza el DataFrame optimizado, guarda en parquet una nueva versión del DataFrame y particionalo por fecha (date). Otra versión por ciudad. Otra por ciudad y fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d51f5320-5b1d-43e7-b7cc-457c22cefaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas en el DataFrame: ['business_id', 'cool', 'date', 'funny', 'review_id', 'stars', 'text', 'useful', 'user_id']\n"
     ]
    }
   ],
   "source": [
    "columnas = df.columns\n",
    "print(\"Columnas en el DataFrame:\", columnas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e4ecf07-19ba-41f9-afb5-7345c8458972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: byte (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- funny: byte (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: float (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: byte (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dc30dcd-e810-46da-84a9-48dfc7ac7bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "py4j does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitionBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/ruta/a/directorio/particionado_por_fecha\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:181\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 181\u001b[0m     converted \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:143\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_instance_of(gw, e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava.lang.IllegalArgumentException\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m IllegalArgumentException(origin\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mis_instance_of\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjava.lang.ArithmeticException\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArithmeticException(origin\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_instance_of(gw, e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava.lang.UnsupportedOperationException\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:464\u001b[0m, in \u001b[0;36mis_instance_of\u001b[0;34m(gateway, java_object, java_class)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava_class must be a string, a JavaClass, or a JavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgateway\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy4j\u001b[49m\u001b[38;5;241m.\u001b[39mreflection\u001b[38;5;241m.\u001b[39mTypeUtil\u001b[38;5;241m.\u001b[39misInstanceOf(\n\u001b[1;32m    465\u001b[0m     param, java_object)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1725\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1722\u001b[0m _, error_message \u001b[38;5;241m=\u001b[39m get_error_message(answer)\n\u001b[1;32m   1723\u001b[0m message \u001b[38;5;241m=\u001b[39m compute_exception_message(\n\u001b[1;32m   1724\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name), error_message)\n\u001b[0;32m-> 1725\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(message)\n",
      "\u001b[0;31mPy4JError\u001b[0m: py4j does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "df.write.partitionBy(\"date\").parquet(\"/ruta/a/directorio/particionado_por_fecha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7d5eb-637a-4955-a346-d6f93d33f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.partitionBy(\"state\").parquet(\"/ruta/a/directorio/particionado_por_ciudad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b978cf4-f5f6-4f15-9b72-87f27f35bb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "df.write.partitionBy(\"date\", \"state\").parquet(\"/ruta/a/directorio/particionado_por_fecha_y_ciudad\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdea8716-1fc5-4be5-ac09-e602651e44fe",
   "metadata": {},
   "source": [
    "6. Ejecuta un query utilizando sobre la tabla filtrando por una de las ciudades y un años en particular. Registra el tiempo de ejecución. Aplica el query sobre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef079cda-313b-4e0d-bca6-75ccaeaf1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "ciudad_filtrar = \"STATE\"  \n",
    "año_filtrar = 2017 \n",
    "\n",
    "df = df.withColumn(\"date\", df.date.cast(\"date\"))\n",
    "filtro = (df.city == ciudad_filtrar) & (df.date.year == año_filtrar)\n",
    "df_filtrado = df.filter(filtro)\n",
    "start_time = time.time()\n",
    "resultados = df_filtrado.show(5)\n",
    "tiempo_ejecucion = time.time() - start_time\n",
    "\n",
    "print(f\"El query se ejecutó en {tiempo_ejecucion:.2f} segundos.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
